{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d846cc",
   "metadata": {},
   "source": [
    "# Chunking for Video Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a93df172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "# Access the environment variables\n",
    "load_dotenv('/Users/williamkapner/Documents/MSBA/GSB570AI/Code/.env')\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "discord_token = os.getenv(\"DISCORD_TOKEN\")\n",
    "\n",
    "\n",
    "boto3_bedrock = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key\n",
    ")\n",
    "\n",
    "# âœ… Now you're safe to use this client in LangChain\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad722b9-9976-4a78-a4d2-9d448b56d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Video_Script_Chunking.ipynb', 'video_chunks.json', 'requiremnets.txt', 'video_script_to_db.ipynb', 'discordQASummerizer.ipynb', 'README.md', 'my_chunks.db', '.gitignore', 'formatted_chunks.json', '.git', 'load_data.py', 'CHAPTER 8 - Organic (MIKE).txt', 'script_ch8.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a500ec28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSBA/GSB570AI/gsb570env/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:56\u001b[39m, in \u001b[36mdependable_faiss_import\u001b[39m\u001b[34m(no_avx2)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# --------- 5. Store in FAISS ---------\u001b[39;00m\n\u001b[32m     44\u001b[39m documents = [Document(page_content=chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# --------- 6. Claude LLM from Bedrock ---------\u001b[39;00m\n\u001b[32m     48\u001b[39m llm = BedrockChat(\n\u001b[32m     49\u001b[39m     client=boto3_session.client(\u001b[33m\"\u001b[39m\u001b[33mbedrock-runtime\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     50\u001b[39m     model_id=\u001b[33m\"\u001b[39m\u001b[33manthropic.claude-3-sonnet-20240229-v1:0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m     model_kwargs={\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1024\u001b[39m}\n\u001b[32m     52\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSBA/GSB570AI/gsb570env/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSBA/GSB570AI/gsb570env/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSBA/GSB570AI/gsb570env/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:996\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__from\u001b[39m(\n\u001b[32m    986\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    994\u001b[39m     **kwargs: Any,\n\u001b[32m    995\u001b[39m ) -> FAISS:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     faiss = \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:\n\u001b[32m    998\u001b[39m         index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSBA/GSB570AI/gsb570env/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:58\u001b[39m, in \u001b[36mdependable_faiss_import\u001b[39m\u001b[34m(no_avx2)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import faiss python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[31mImportError\u001b[39m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import os\n",
    "\n",
    "load_dotenv('/Users/williamkapner/Documents/MSBA/GSB570AI/Code/.env')\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "\n",
    "# --------- 1. Setup AWS Credentials ---------\n",
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=aws_region  # or your Bedrock-supported region\n",
    ")\n",
    "\n",
    "# --------- 2. Load and Preprocess Your Script File ---------\n",
    "# If your uploaded .ipynb is misnamed and actually a .csv or text:\n",
    "try:\n",
    "    df = pd.read_csv(\"CHAPTER 8 - Organic (MIKE).txt\", engine=\"python\")\n",
    "    all_text = \" \".join(df.astype(str).values.flatten())\n",
    "except Exception:\n",
    "    with open(\"CHAPTER 8 - Organic (MIKE).txt\", \"r\") as f:\n",
    "        all_text = f.read()\n",
    "\n",
    "# --------- 3. Chunk the Text ---------\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "# --------- 4. Titan Embeddings from Bedrock ---------\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=boto3_session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# --------- 5. Store in FAISS ---------\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "# --------- 6. Claude LLM from Bedrock ---------\n",
    "llm = BedrockChat(\n",
    "    client=boto3_session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_tokens\": 1024}\n",
    ")\n",
    "\n",
    "# --------- 7. Create RetrievalQA Chain ---------\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# --------- 8. Ask a Question ---------\n",
    "question = \"What makes a viral video convert?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# --------- 9. Display Result ---------\n",
    "print(\"\\nAnswer:\\n\", result[\"result\"])\n",
    "print(\"\\nRelevant Script Sections:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7f46f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9841f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def fixed_size_chunking(text, chunk_size, overlap):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += chunk_size - overlap  # move start forward with overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33956f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e034610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690992b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "file_path = \"StudyDropshipping-ChatBot/CHAPTER 8 - Organic (MIKE).txt\"\n",
    "home_care_content = read_file(file_path)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "semantic_chunks = semantic_embedding_chunk(home_care_content, threshold=0.5)\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf674d-c3e6-47c4-ae34-57706ed8e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = \"/Users/williamkapner/Downloads/CHAPTER 7 - Mindset.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_script = f.read()\n",
    "\n",
    "# Step 1: Normalize headers and section breaks\n",
    "def preprocess_script(text):\n",
    "    # Normalize line breaks\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text.strip())\n",
    "    return text\n",
    "\n",
    "# Step 2: Split into chunks by detecting headers (assuming each header is followed by a paragraph)\n",
    "def chunk_script(text):\n",
    "    chunks = []\n",
    "    pattern = r'(?:^|\\n{2,})([A-Z][A-Za-z0-9\\s]+)\\n+(.+?)(?=\\n{2,}[A-Z]|\\Z)'  # Header + Paragraph until next header or end\n",
    "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "\n",
    "    for i, (header, body) in enumerate(matches):\n",
    "        chunk = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"header\": header.strip(),\n",
    "            \"body\": body.strip()\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Format for LLM ingestion or output as JSON/text\n",
    "def print_chunks(chunks):\n",
    "    for chunk in chunks:\n",
    "        print(f\"### Chunk {chunk['chunk_id']}: {chunk['header']}\\n\")\n",
    "        print(chunk['body'])\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Usage\n",
    "processed = preprocess_script(raw_script)\n",
    "chunked = chunk_script(processed)\n",
    "print_chunks(chunked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60aebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assume `chunked` is already created from earlier steps (a list of dictionaries)\n",
    "# Each element looks like: { 'chunk_id': 1, 'header': ..., 'body': ... }\n",
    "\n",
    "# Step 1: Convert to desired JSON format\n",
    "formatted_chunks = []\n",
    "for i, chunk in enumerate(chunked, start=1):\n",
    "    formatted_chunk = {\n",
    "        \"id\": i,\n",
    "        \"header\": chunk[\"header\"],\n",
    "        \"content\": chunk[\"body\"],\n",
    "        \"category\": 'Organic Marketing',\n",
    "        \"source\": 'course'\n",
    "    }\n",
    "    formatted_chunks.append(formatted_chunk)\n",
    "\n",
    "# Step 2: (Optional) Save to JSON file\n",
    "output_path = \"formatted_chunks.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(formatted_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Step 3: (Optional) Print first few for verification\n",
    "for fc in formatted_chunks[:5]:  # Just preview the first 3\n",
    "    print(json.dumps(fc, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# Load your chunked JSON\n",
    "with open(\"formatted_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# Construct a context string from all chunks\n",
    "def build_context(chunks):\n",
    "    context = \"\"\n",
    "    for chunk in chunks:\n",
    "        context += f\"\\n\\n### {chunk['chunk_header']}\\n{chunk['content']}\\n\"\n",
    "    return context.strip()\n",
    "\n",
    "# Ask a question using Claude via Bedrock\n",
    "def ask_bedrock_claude(question, chunks, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"):\n",
    "    context = build_context(chunks)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the CONTEXT below to answer the QUESTION at the end.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\", \"\\n\\n\"],\n",
    "    }\n",
    "\n",
    "    response = client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"body\"].read())\n",
    "    return result.get(\"completion\", \"\").strip()\n",
    "\n",
    "# Example usage\n",
    "question = \"What is the difference between organic and paid advertising?\"\n",
    "answer = ask_bedrock_claude(question, chunks)\n",
    "print(\"\\nAnswer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/ytdznf2d6mxcx8d7tp5l1qb40000gn/T/ipykernel_92395/4289400057.py:37: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# âœ… Step 1: Load chunks from JSON\n",
    "with open(\"formatted_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# âœ… Step 2: Convert to LangChain Documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=chunk[\"content\"],\n",
    "        metadata={\"chunk_header\": chunk[\"chunk_header\"], \"paragraph_number\": chunk[\"paragraph_number\"]}\n",
    "    )\n",
    "    for chunk in chunks\n",
    "]\n",
    "\n",
    "# âœ… Step 3: Set up Bedrock Titan Embeddings with credentials\n",
    "boto3_bedrock = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key\n",
    ")\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# âœ… Step 4: Create and persist Chroma VectorStore\n",
    "vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory=\"./dropshipping_vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"âœ… Vector store created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# Load your chunked JSON\n",
    "with open(\"formatted_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# Construct a context string from all chunks\n",
    "def build_context(chunks):\n",
    "    context = \"\"\n",
    "    for chunk in chunks:\n",
    "        context += f\"\\n\\n### {chunk['chunk_header']}\\n{chunk['content']}\\n\"\n",
    "    return context.strip()\n",
    "\n",
    "# Ask a question using Claude via Bedrock\n",
    "def ask_bedrock_claude(question, chunks, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"):\n",
    "    context = build_context(chunks)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the CONTEXT below to answer the QUESTION at the end.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\", \"\\n\\n\"],\n",
    "    }\n",
    "\n",
    "    response = client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"body\"].read())\n",
    "    return result.get(\"completion\", \"\").strip()\n",
    "\n",
    "# Example usage\n",
    "question = \"What is the difference between organic and paid advertising?\"\n",
    "answer = ask_bedrock_claude(question, chunks)\n",
    "print(\"\\nAnswer:\\n\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsb570env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

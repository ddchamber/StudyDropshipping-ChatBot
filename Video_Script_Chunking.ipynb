{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d846cc",
   "metadata": {},
   "source": [
    "# Chunking for Video Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad722b9-9976-4a78-a4d2-9d448b56d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ttest.py', 'Video_Script_Chunking.ipynb', 'requiremnets.txt', 'discordQASummerizer.ipynb', 'README.md', '.gitignore', 'discordQASummerizer.py', '.env', '.git', 'CHAPTER 8 - Organic (MIKE).txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a500ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/kkh608tn0k194f6cv0bwbqp80000gn/T/ipykernel_58890/3647972443.py:38: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import BedrockEmbeddings``.\n",
      "  embeddings = BedrockEmbeddings(\n",
      "/var/folders/jl/kkh608tn0k194f6cv0bwbqp80000gn/T/ipykernel_58890/3647972443.py:48: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import ChatBedrock``.\n",
      "  llm = BedrockChat(\n",
      "/var/folders/jl/kkh608tn0k194f6cv0bwbqp80000gn/T/ipykernel_58890/3647972443.py:64: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " Based on the context provided, it seems that for a viral video to \"convert\" (presumably meaning to lead to some desired action like making a purchase or signing up), the key is to evoke an emotional reaction from the viewer.\n",
      "\n",
      "The examples mention that viral videos often strike an emotional chord, whether that's making people laugh, feel angry/outraged, confused, etc. Evoking those strong emotions gets viewers engaged and compels them to like, comment, share the video, and potentially take a next step like buying a product or service.\n",
      "\n",
      "However, the context doesn't provide specific details on what types of emotional responses are most effective for driving conversions versus just getting views and engagement. It likely depends on the specific goal and call-to-action of the video campaign.\n",
      "\n",
      "Relevant Script Sections:\n",
      "\n",
      "Examples of viral videos that don't convert and do convert\n",
      "of how organic works and where we will be posting it's time to talk about what actually makes a video go viral.\n",
      "common theme for a viral video is striking some sort of emotion in the viewer. You need them to feel something when watching your video. This has many layers to it because there are so many emotions that us humans can feel. Your video could make them angry which leads them to comment and engage with the post. It could be funny which leads them to like, save, and share it with someone. It could even be confusing which then leads the viewer to have to watch the whole video through to understand\n",
      "Examples of our personal viral videos and why they did well\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "\n",
    "# --------- 1. Setup AWS Credentials ---------\n",
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=aws_region  # or your Bedrock-supported region\n",
    ")\n",
    "\n",
    "# --------- 2. Load and Preprocess Your Script File ---------\n",
    "# If your uploaded .ipynb is misnamed and actually a .csv or text:\n",
    "try:\n",
    "    df = pd.read_csv(\"CHAPTER 8 - Organic (MIKE).txt\", engine=\"python\")\n",
    "    all_text = \" \".join(df.astype(str).values.flatten())\n",
    "except Exception:\n",
    "    with open(\"CHAPTER 8 - Organic (MIKE).txt\", \"r\") as f:\n",
    "        all_text = f.read()\n",
    "\n",
    "# --------- 3. Chunk the Text ---------\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "# --------- 4. Titan Embeddings from Bedrock ---------\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=boto3_session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# --------- 5. Store in FAISS ---------\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "# --------- 6. Claude LLM from Bedrock ---------\n",
    "llm = BedrockChat(\n",
    "    client=boto3_session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_tokens\": 1024}\n",
    ")\n",
    "\n",
    "# --------- 7. Create RetrievalQA Chain ---------\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# --------- 8. Ask a Question ---------\n",
    "question = \"What makes a viral video convert?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# --------- 9. Display Result ---------\n",
    "print(\"\\nAnswer:\\n\", result[\"result\"])\n",
    "print(\"\\nRelevant Script Sections:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb7f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9841f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def fixed_size_chunking(text, chunk_size, overlap):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += chunk_size - overlap  # move start forward with overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33956f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e034610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43a2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_chunking(text):\n",
    "    # Simple split by double line breaks (standard for paragraphs)\n",
    "    paragraphs = [para.strip() for para in text.split('\\n\\n\\n') if para.strip()]\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9930810d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# open file and read text from file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudyDropshipping/CHAPTER 8 - Organic (MIKE).txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m file_content \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m(file_path)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to read data from file: \u001b[39m\u001b[38;5;124m\"\u001b[39m, file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_file' is not defined"
     ]
    }
   ],
   "source": [
    "# open file and read text from file\n",
    "# Example usage\n",
    "file_path = \"studyDropshipping/CHAPTER 8 - Organic (MIKE).txt\"\n",
    "file_content = read_file(file_path)\n",
    "\n",
    "if file_content is None:\n",
    "    print(\"Unable to read data from file: \", file_path)\n",
    "\n",
    "# Generate chunks\n",
    "paragraph_chunks = paragraph_chunking(file_content)\n",
    "\n",
    "# Display results\n",
    "for i, chunk in enumerate(paragraph_chunks):\n",
    "    print(f\"\\n--- Chunk {i + 1} ---\\n{chunk}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

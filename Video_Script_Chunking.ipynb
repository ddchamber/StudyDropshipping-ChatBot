{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d846cc",
   "metadata": {},
   "source": [
    "# Chunking for Video Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad722b9-9976-4a78-a4d2-9d448b56d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EmbeddingAssignment.ipynb', 'Labs', '.ipynb_checkpoints', 'api-keys', 'test_script', 'Video_Script_Chunking.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a500ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164/3047883453.py:48: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import ChatBedrock``.\n",
      "  llm = BedrockChat(\n",
      "/tmp/ipykernel_164/3047883453.py:64: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " The video does not mention anything about prompting best practices. It is focused on providing tips for launching a dropshipping product, such as finding a product that solves a problem or follows a trend, using tools like Google Trends to identify popular topics, finding reliable suppliers, and ordering product samples. There is no discussion about prompting best practices in this context.\n",
      "\n",
      "Relevant Script Sections:\n",
      "\n",
      "Hey everyone, welcome back! In today’s video, I’m going to walk you through exactly how to launch your very first dropshipping product — step by step, no fluff.\n",
      "Let’s start with product selection. The first thing you need to do is find a product that actually solves a real problem or hits on a trend. I like to use tools like Google Trends or even scroll through TikTok and Instagram to see what’s getting a lot of engagement. You want something people are already talking about — it makes your marketing job ten times easier.\n",
      "So there you have it — that’s the basic blueprint for launching your first dropshipping product. Don’t overthink it, just take action, learn as you go, and don’t be afraid to make mistakes. Hit the like button if this helped, and I’ll see you in the next one.\n",
      "Once you’ve picked a potential product, it’s time to find a reliable supplier. Most people head straight to AliExpress, which is fine to start, but I also recommend checking platforms like Spocket or Zendrop because they often offer faster shipping and better communication. Always, always order a sample — you want to know exactly what your customer is getting.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "\n",
    "# --------- 1. Setup AWS Credentials ---------\n",
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=aws_region  # or your Bedrock-supported region\n",
    ")\n",
    "\n",
    "# --------- 2. Load and Preprocess Your Script File ---------\n",
    "# If your uploaded .ipynb is misnamed and actually a .csv or text:\n",
    "try:\n",
    "    df = pd.read_csv(\"test_script\", engine=\"python\")\n",
    "    all_text = \" \".join(df.astype(str).values.flatten())\n",
    "except Exception:\n",
    "    with open(\"test_script\", \"r\") as f:\n",
    "        all_text = f.read()\n",
    "\n",
    "# --------- 3. Chunk the Text ---------\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "# --------- 4. Titan Embeddings from Bedrock ---------\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=boto3_session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# --------- 5. Store in FAISS ---------\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "# --------- 6. Claude LLM from Bedrock ---------\n",
    "llm = BedrockChat(\n",
    "    client=boto3_session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_tokens\": 1024}\n",
    ")\n",
    "\n",
    "# --------- 7. Create RetrievalQA Chain ---------\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# --------- 8. Ask a Question ---------\n",
    "question = \"What does the video say about prompting best practices?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# --------- 9. Display Result ---------\n",
    "print(\"\\nAnswer:\\n\", result[\"result\"])\n",
    "print(\"\\nRelevant Script Sections:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eb7f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9841f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def fixed_size_chunking(text, chunk_size, overlap):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += chunk_size - overlap  # move start forward with overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33956f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e034610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "690992b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Hey everyone , welcome back ! In today ’s video , I ’m going to walk you through exactly how to launch your very first dropshipping product — step by step , no fluff . \n",
      "\n",
      " Let ’s start with product selection . The first thing you need to do is find a product that actually solves a real problem or hits on a trend . I like to use tools like Google Trends or even scroll through TikTok and Instagram to see what ’s getting a lot of engagement . You want something people are already talking about — . You want something people are already talking about — it makes your marketing job ten times easier . \n",
      "\n",
      " Once you ’ve picked a potential product , it ’s time to find a reliable supplier . Most people head straight to AliExpress , which is fine to start , but I also recommend checking platforms like Spocket or Zendrop because they often offer faster shipping and better communication . Always , always order a sample — you want to know exactly what your customer is getting . \n",
      "\n",
      " Now let ’s talk about your store . For beginners , let ’s talk about your store . For beginners , Shopify is hands down the easiest platform to get started with . Just pick a clean theme — do n’t overcomplicate it — and make sure it looks good on mobile because that ’s where most of your traffic is going to come from . Add your product , write a description that clearly explains what it does and why it ’s awesome , and include some high - quality images or videos if you can . \n",
      "\n",
      " Payment setup is pretty simple : just connect PayPal or Stripe , is pretty simple : just connect PayPal or Stripe , or both if possible . When it comes to shipping , be transparent . Let your customers know how long it ’s going to take , and use automated emails to keep them updated every step of the way . \n",
      "\n",
      " Alright , now for the fun part — marketing . I usually recommend starting with Facebook or Instagram ads . Create a short , scroll - stopping video ad and test different audiences . If you ’ve got a bit of a budget , influencer shoutouts on TikTok can bit of a budget , influencer shoutouts on TikTok can also work really well . The key here is to start small , test fast , and double down on what works . \n",
      "\n",
      " So there you have it — that ’s the basic blueprint for launching your first dropshipping product . Do n’t overthink it , just take action , learn as you go , and do n’t be afraid to make mistakes . Hit the like button if this helped , and I ’ll see you in the next one . \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Chunk 2:\n",
      "next one . \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "file_path = \"./test_script\"\n",
    "home_care_content = read_file(file_path)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "semantic_chunks = semantic_embedding_chunk(home_care_content, threshold=0.5)\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf674d-c3e6-47c4-ae34-57706ed8e981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
